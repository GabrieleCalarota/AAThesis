\chapter{Authorship attribution's tasks}

\epigraph{\textit{"Science is the systematic classification of experience."}\\George Henry Lewes}

The task of determining or verifying the authorship of an anonymous text based solely on internal evidence is a very old one, dating back at least to the medieval scholastics,
for whom the reliable attribution of a given text to a known ancient authority was essential to determining the text’s veracity. More recently, this problem of authorship attribution has gained greater prominence due to new applications in forensic analysis, humanities scholarship, and electronic commerce, and the development of computational methods for addressing the problem.
Statistical authorship attribution has a long history, culminating in the use of modern machine learning classification methods.
In the simplest form of the problem, we are given examples of the writing of a number of candidate authors and are asked to determine which of them authored a given anonymous
text. In this straightforward form, the authorship attribution problem fits the standard modern paradigm of a text categorization problem \cite{lewis1994comparison} \cite{sebastiani2002machine}. The components of text categorization systems are by
now fairly well understood: Documents are represented as numerical vectors that capture statistics of potentially relevant features of the text, and machine learning methods are
used to find classifiers that separate documents that belong to different classes.
In the next section we will briefly present the approach to this task in the era before machine learning. Later, we will discuss about the different approaches of this particular tasks, whether to approach it as a profile-based or an instance-based, depending on which domain are we tackling this task: single-domain or cross-domain.
In the last section, we will introduce to the latest state of the art approaches for this particular classification task. 

\section{History of methodologies}

The first attempts to quantify the writing style go back to the 19th century, with the pioneering study of Mendenhall (1887) on the plays of Shakespeare, followed
by statistical studies in the first half of the 20th century by Yule (1938, 1944) and Zipf (1932) \cite{williams1975mendenhall} \cite{yule1938test} \cite{zipf1932selected}. Later, the detailed study by Mosteller and Wallace (1964) on the authorship of “The
Federalist Papers” (a series of 146 political essays written by John Jay, Alexander Hamilton, and James Madison, 12 of which claimed by both Hamilton and Madison) was undoubtedly the most influential work in authorship attribution \cite{stamatatos2009survey}. Their method was based on Bayesian statistical analysis of the frequencies of a small set of common words (e.g., “and,” “to,” etc.) and produced significant discrimination results between the candidate authors.
Essentially, the work of Mosteller and Wallace (1964) initiated nontraditional authorship attribution studies, as opposed to traditional human-expert-based methods. Since
then and until the late 1990s, research in authorship attribution was dominated by attempts to define features for quantifying writing style, a line of research known as \textit{“stylometry”} \cite{holmes1998evolution}. Hence, a great variety of measures, including sentence length, word length, word frequencies, character frequencies, and vocabulary richness functions, had been proposed. Rudman (1998) estimated that
nearly 1,000 different measures had been proposed by the late 1990s \cite{rudman1997state}. The authorship attribution methodologies proposed during that period were computer-assisted rather than
computer-based, meaning that the aim was rarely at developing a fully automated system. In certain cases, there were methods that achieved impressive preliminary results and
made many people think that the solution of this problem was too close.
The most characteristic example is the CUSUM (or QSUM) technique \cite{michaelson1990qsum} that gained publicity and was accepted in courts as expert evidence; however, the research community heavily criticized it and considered it generally unreliable \cite{holmes1995forensic}. Actually, the main problem of that early period was the lack of objective evaluation of the proposed methods. In most of the cases, the testing ground was literary works of unknown or disputed authorship (e.g., the Federalist Papers
case), so the estimation of attribution accuracy was not even possible. The main methodological limitations of that period concerning the evaluation procedure were the following:
\begin{itemize}
	\item The textual data were too long (usually including entire books) and probably not stylistically homogeneous.
	\item The number of candidate authors was too small (usually two or three).
	\item The evaluation corpora were not controlled for topic.
	\item The evaluation of the proposed methods was mainly intuitive (usually based on subjective visual inspection of scatterplots).
	\item The comparison of different methods was difficult due to lack of suitable benchmark data.
\end{itemize}

Since the late 1990s, things have changed in authorship attribution studies. The vast amount of electronic texts available through Internet media (e-mail messages, blogs, online
forums, etc.) have increased the need for efficiently handling this information. This fact had a significant impact in scientific areas such as information retrieval, machine learning,
and natural language processing (NLP). The development of these areas influenced authorship attribution technology as described:
\begin{itemize}
	\item Information retrieval research developed efficient techniques for representing and classifying large volumes of text.
	\item Powerful machine learning algorithms became available to handle multidimensional and sparse data, allowing more expressive representations. Moreover, standard evaluation
	methodologies have been established to compare different approaches on the same benchmark data.
	\item NLP research developed tools able to analyze text efficiently and provided new forms of measures for representing the style (e.g., syntax-based features).
\end{itemize}

More importantly, the plethora of available electronic texts revealed the potential of authorship analysis in various applications \cite{madigan2005author} in diverse areas including intelligence (e.g., attribution of messages or proclamations to known terrorists,
linking different messages by authorship) \cite{abbasi2005applying}, criminal law (e.g., identifying writers of harassing messages, verifying the authenticity of suicide notes) and
civil law (e.g., copyright disputes) \cite{chaski2005s}, and computer forensics (e.g., identifying the authors of source code of malicious software) \cite{frantzeskou2006effective} in addition to the traditional application to literary research (e.g., attributing anonymous or disputed literary works to known authors) \cite{burrows2002delta}. Hence, (roughly) the last decade can be viewed as a new era of authorship analysis technology, this time dominated by efforts to develop practical applications dealing with real-world texts (e.g., e-mail messages, blogs,
online forum messages, source code, etc.) rather than solving disputed literary questions. Emphasis has now been given to the objective evaluation of the proposed methods as well
as the comparison of different methods based on common benchmark corpora. In addition, factors playing a crucial role in the accuracy of the produced models are examined, such as the training text size \cite{marton2005compression}, the number
of candidate authors \cite{koppel2006authorship}, and the distribution of training texts over the candidate authors \cite{stamatatos2008author}.

\section{Training's approach}

In every authorship-identification problem, there is a set of candidate authors, a set of text samples of known authorship covering all the candidate authors (\textit{training corpus}), and a set of text samples of unknown authorship (\textit{test corpus}); each
one of them should be attributed to a candidate author. In this survey, we distinguish the authorship attribution approaches according to whether they treat each training text individually or cumulatively (per author). In more detail, some approaches concatenate all the available training texts per author in one big file and extract a cumulative representation of that author’s style (usually called the author’s profile) from
this concatenated text. That is, the differences between texts
written by the same author are disregarded. Such approach just described is called \textit{"profile-based approach"} and early work in authorship attribution has followed this practice \cite{mosteller2007inference}.
On the other hand, another family of approaches requires multiple training text samples per author to develop an accurate attribution model. That is, each training text is
individually represented as a separate instance of authorial style. Such \textit{instance-based approaches}\footnote{Note that this term should not be confused with instance-based learning methods\cite{mitchell1997artificial}} are described in the
next section, followed by hybrid approaches attempting to combine characteristics of profile-based and instance-based methods. We then compare these two basic approaches and
discuss their strengths and weaknesses across several factors.

\subsection{Profile-based approach}

One way to handle the available training texts per author is to concatenate them in one single text file. This large file is used to extract the properties of the author’s style. An unseen text is, then, compared with each author file, and the most likely author is estimated based on a distance measure. It should be stressed that there is no separate representation of each text sample but only one representation of a large file per author. As a result, the differences between the training texts by the same author are disregarded. Moreover, the stylometric measures extracted from the concatenated file may be quite
different in comparison to each of the original training texts.

\begin{figure}[ht]
	\centering
	\includegraphics[width=.8\textwidth, height=.6\textheight, keepaspectratio]{profile-based-approach}
	\caption[Profile-based approach]{Typical architecture of profile-based in authorship attribution approaches}
	\label{fig:profile_based_approach}
\end{figure}

A typical architecture of a profile-based approach is depicted in figure  \ref{fig:profile_based_approach}. Note that $x$ denotes a vector of text representation
features. Hence, $x_A$ is the profile of Author $A$, and $x_u$ is the
profile of the unseen text.
The profile-based approaches have a very simple training process. Actually, the training phase just comprises the extraction of profiles for the candidate authors. Then, the attribution model is usually based on a distance function that computes the differences of the profile of an unseen text and the profile of each author. Let $PR(x)$ be the profile of text $x$ and $d[PR(x),PR(y)]$ the distance between the profile of text $x$ and the profile of text $y$. Then, the most likely author of an unseen text $x$ is given in \ref{autor_equation_1}, where $A$ is the set of candidate authors and $x_a$ is the concatenation of all training texts for author $a$.
\begin{equation}\label{autor_equation_1}
author(x) = \arg_{a \in A} min\ d(PR(x), PR(x_a)
\end{equation}


\subsection{Instance-based approach}

The majority of the modern authorship-identification approaches considers each training text sample as a unit that contributes separately to the attribution model. In other words, each text sample of known authorship is an instance of the problem in question. A typical architecture of such an instance-based approach is shown in figure  \ref{fig:instance_based_approach}. In detail, each text sample of the training corpus is represented by a vector of attributes $(x)$ following methods described earlier, and a
classification algorithm is trained using the set of instances of known authorship (\textit{training set}) to develop an attribution model. Then, this model will be able to estimate the true author of an unseen text.

\begin{figure}[ht]
	\centering
	\includegraphics[width=.8\textwidth, height=.6\textheight, keepaspectratio]{instance-based-approach}
	\caption[Instance-based approach]{Typical architecture of instance-based in authorship attribution approaches}
	\label{fig:instance_based_approach}
\end{figure}

Note that such classification algorithms require multiple training instances per class for extracting a reliable model. Therefore, according to instance-based approaches, in case
we have only one, but a quite long, training text for a particular candidate author (e.g., an entire book), this should be segmented into multiple parts, probably of equal length.
From another point of view, when there are multiple training text samples of variable length per author, the training text instance length should be normalized. To that end, the training texts per author are segmented to equal-sized samples \cite{sanderson2006short}. In all these cases, the text samples should be long enough so that the text representation features can adequately represent their style. Various lengths of text samples have been reported in the literature. Sanderson and Guenter (2006) produced chunks of 500 characters \cite{sanderson2006short}. \citeauthor{koppel2006authorship} (\citeyear{koppel2006authorship}) segmented the training texts into
chunks of about 500 words \cite{koppel2006authorship}. \citeauthor{hirst2007bigrams} (\citeyear{hirst2007bigrams}) conducted experiments with text blocks of varying length (i.e.: 200, 500, and 1000 words) and reported significantly reduced accuracy as the text-block length decreases \cite{hirst2007bigrams}. Therefore, the choice of the training instance text sample is not a trivial process and directly affects the performance of the attribution model.


\section{The real problem}

Statistical authorship attribution has a long history, culminating in the use of modern machine learning classification methods. Nevertheless, most of this work suffers from the limitation of assuming a small closed set of candidate authors and essentially unlimited training text for each. Real-life authorship attribution problems, however, typically fall short of this ideal. Thus, following detailed discussion of previous work, three scenarios are considered here for which solutions to the basic attribution problem are inadequate.
For example, we may encounter scenarios such as:

\begin{enumerate}
	\item There is no candidate set at all. In this case, the challenge is to provide as much demographic or psychological information as possible about the author. This is the \textit{profiling problem}.
	\item There are many thousands of candidates for each of whom we might have a very limited writing sample. This is the \textit{needle-in-a-haystack} problem.
	\item There is no closed candidate set but there is one suspect. In this case, the challenge is to determine if the suspect is or is not the author. This is the \textit{verification problem}.
\end{enumerate}

In the following section we will show how machine learning methods can be adapted to handle the special challenges of each variant.

\subsection{Profiling problem}
As noted previously, even in cases where we have an anonymous text and no candidate authors, we would like to say something about the anonymous author. That is, we wish to exploit the sociolinguistic observation that different groups of people speaking or writing in a particular genre and in a particular language use that language differently \cite{chambers2004handbook}. More specifically, we wish to use the features and methods employed earlier to distinguish between individual authors to distinguish between
classes of authors.

\subsection{Needle-in-hay-stack problem}

Consider now the scenario where we seek to determine the specific identity of a document’s author, but there are many thousands of potential candidates. We call this the \textit{needle-in-a-haystack} attribution problem. In this case, standard text-classification techniques are unlikely to give reasonable accuracy and may require excessive computation time to learn classification models; however, we will show in this section that if we are willing to tolerate our system telling us it does not know the answer, we can achieve high accuracy for the cases where the system does give us an attribution it consider reliable.

\subsection{Verification problem}
 
Consider the case in which we are given examples of the writing of a single author and are asked to verify that a given target text was or was not written by this author. As a categorization problem, verification is significantly more difficult than basic attribution, and virtually no work has been done on it (but see \citeauthor{halteren2004linguistic} \citeyear{halteren2004linguistic}), outside the framework of plagiarism detection \citeauthor{zu2007plagiarism} \citeyear{zu2007plagiarism}. If, for example, all we wished to do is to determine if a text had been written by Shakespeare or by Marlowe, it would be sufficient to use their respective known writings, to construct a model distinguishing them, and to test the unknown text against the model. If, on the other hand, we need
to determine if a text was written by Shakespeare, it is difficult to assemble a representative sample of non-Shakespeare texts.
The situation in which we suspect that a given author may have written some text, but do not have an exhaustive list of alternative candidates, is a common one \cite{koppel2009computational}. The problem is complicated by the fact that a single author may vary his or her style from text to text or may unconsciously drift stylistically over time, not to mention the possibility of conscious deception. Thus, we must learn to somehow distinguish between relatively shallow differences that reflect conscious or unconscious changes in an author’s style and deeper differences that reflect styles of different authors.