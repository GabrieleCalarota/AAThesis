\chapter{Related Work}

\cite{antoniak2018evaluating}
\cite{word2vecFastText}
In Natural Language Processing (NLP), we often map words into vectors that contains numeric values so that machine can understand it. Word embedding is a type of mapping that allows words with similar meaning to have similar representation.

\section{Traditional Approach}

A traditional way of representing words is one-hot vector, which is essentially a vector with only one target element being 1 and the others being 0. The length of the vector is equal to the size of the total unique vocabulary in the corpora. Conventionally, these unique words are encoded in alphabetical order. Namely, you should expect the one-hot vectors for words starting with “a” with target “1” of lower index, while those for words beginning with “z” with target “1” of higher index.


\begin{figure}[ht]
    \centering
    \includegraphics[width=.6\textwidth, height=.4\textheight, keepaspectratio]{1_hot_vector}
    \caption[One Hot vector]{A simple and easy way to implement word vectors.}
    \label{fig:1_hot_vector}
\end{figure}

Though this representation of words is simple and easy to implement, there are several issues. First, you cannot infer any relationship between two words given their one-hot representation. For instance, the word “endure” and “tolerate”, although have similar meaning, their targets “1” are far from each other. In addition, sparsity is another issue as there are numerous redundant “0” in the vectors. This means that we are wasting a lot of space.We need a better representation of words to solve these issues.

\section{Word2Vec}

Word2Vec is an efficient solution to these problems, which leverages the context of the target words. Essentially, we want to use the surrounding words to represent the target words with a Neural Network whose hidden layer encodes the word representation.

There are two types of Word2Vec, Skip-gram and Continuous Bag of Words (CBOW). I will briefly describe how these two methods work in the following paragraphs.

\subsection{Skip-gram}

For skip-gram, the input is the target word, while the outputs are the words surrounding the target words. For instance, in the sentence “I have a cute dog”, the input would be “a”, whereas the output is “I”, “have”, “cute”, and “dog”, assuming the window size is 5. All the input and output data are of the same dimension and one-hot encoded. The network contains 1 hidden layer whose dimension is equal to the embedding size, which is smaller than the input/ output vector size. At the end of the output layer, a softmax activation function is applied so that each element of the output vector describes how likely a specific word will appear in the context. The graph below visualizes the network structure.

\begin{figure}[ht]
	\centering
	\includegraphics[width=.4\textwidth, height=.2\textheight, keepaspectratio]{skip_gram}
	\caption[Skip-gram]{The word embedding for the target words can obtained by extracting hidden layers after feeding the one-hot representation of that word into the network.}
	\label{fig:skip_gram}
\end{figure}

With skip-gram, the representation dimension decreases from the vocabulary size (V) to the length of the hidden layer (N). Furthermore, the vectors are more “meaningful” in terms of describing the relationship between words. The vectors obtained by subtracting two related words sometimes express a meaningful concept such as gender or verb tense, as shown in the following figure (dimensionality reduced).

\begin{figure}[ht]
	\centering
	\includegraphics[width=.6\textwidth, height=.4\textheight, keepaspectratio]{skip_gram_2}
	\caption[Skip-gram]{The vectors are more "meaningful" in terms of describing the relationship between words.}
	\label{fig:skip_gram_2}
\end{figure}

\subsection{CBOW: Continuos Bag of Words}

Continuous Bag of Words (CBOW) is very similar to skip-gram, except that it swaps the input and output. The idea is that given a context, we want to know which word is most likely to appear in it.

\begin{figure}[ht]
	\centering
	\includegraphics[width=.6\textwidth, height=0.4\textheight, keepaspectratio]{cbow}
	\caption[CBOW]{The main difference between CBOW and Skip-gram is that CBOW swaps the input and output.}
	\label{fig:cbow}
\end{figure}

The biggest difference between Skip-gram and CBOW is that the way the word vectors are generated. For CBOW, all the examples with the target word as target are fed into the networks, and taking the average of the extracted hidden layer. For example, assume we only have two sentences, “He is a nice guy” and “She is a wise queen”. To compute the word representation for the word “a”, we need to feed in these two examples, “He is nice guy”, and “She is wise queen” into the Neural Network and take the average of the value in the hidden layer. Skip-gram only feed in the one and only one target word one-hot vector as input.

It is claimed that Skip-gram tends to do better in rare words. Nevertheless, the performance of Skip-gram and CBOW are generally similar.

\section{FastText}

FastText is an extension to Word2Vec proposed by Facebook in 2016. Instead of feeding individual words into the Neural Network, FastText breaks words into several n-grams (sub-words). For instance, the tri-grams for the word apple is app, ppl, and ple (ignoring the starting and ending of boundaries of words). The word embedding vector for apple will be the sum of all these n-grams. After training the Neural Network, we will have word embeddings for all the n-grams given the training dataset. Rare words can now be properly represented since it is highly likely that some of their n-grams also appears in other words. I will show you how to use FastText with Gensim in the following section.