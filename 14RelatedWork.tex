\chapter{Techniques and data collection for Authorship Attribution}

\epigraph{\enquote*{\textit{State of the Art is the frenetic and relentless pursuit of doing what its best at that time!}}\\
Da Anunciaç\~ao Marco}

There are different types of authorship attribution studies in the literature such
as predicting the date of authorship of historical texts or text genre detection \cite{tausz2011predicting}, \cite{kessler1997automatic}.
Vast majority of previous works focuses on authorship identification by taking into
consideration the stylistic features of authors such as use of grammar, function words,
frequent word allocations \cite{argamon2005measuring}, \cite{fox2012statistical}, \cite{hirst2007bigrams}. Some of the well-known problems
in authorship attribution are disputed Federalist Papers classification and Shakespearean Authorship Dispute.
The Federalist Papers are a collection of 85 articles and essays written by Alexander Hamilton, James Madison, and John Jay to persuade the citizens of New York
to ratify the U.S. Constitution. Authorship of twelve of these papers has been in
dispute. To address this problem, using linear support vector machines as classifier
and relative frequencies of words as features a study identified these papers to be
written by James Madison \cite{mosteller2007inference}.\\
Another dispute in authorship attribution among scholars across the world is
whether William Shakespeare wrote the works attributed to him or not. It was argued
that Shakespeare was not even educated and more than 80 authors were suggested to
be the author of the writings that were under the name of Shakespeare. Christopher
Marlowe is considered the most likely candidate to write these works under the name
of Shakespeare when he was in jail. In order to analyze the stylistic fingerprint of
Shakespeare and Marlowe and non-Shakespearean authors, namely Chapman, Jonson, Middleton, a corpus has been put together \cite{fox2012statistical}.\\
The classification results for non-Shakespearean author candidates turned out to
be highly accurate (Johnson \%100, Chapman \%92.9 and Middleton \%88.9). The
results supported the hypothesis that writing styles of Marlowe and Shakespeare were
as distinguishable as other authors unless Marlowe did not show a linear change in
style over time. Meaning, Marlowe has found not to be the authors of Shakespearean
writings.
Another interesting study on the unknown texts is also
done based on word-level features, vocabulary richness and syntactic features by using
Liblinear SVM for classification purposes \cite{stanko2013whose}. Even though the classification accuracy
results are not as high as other related works features like \enquote*{number of unique words}
should be noted for use in any attribution problem.\\
Usefulness of function words in authorship attribution is introduced by Mosteller
and Wallace in their work on Federalist papers \cite{mosteller2007inference}. Argamon and Levitan has compared the characteristic features of frequent words, pairs and collocations using the
SMO algorithm, and implemented it for two class (American or British) author nationality classification problem. Their results conclude that function words are useful
as stylistic text attribution and frequent words are the best features among others.
The reason behind it is that a given same size frequent collocations has less different
words comparing to frequent words so it carries less discriminatory features \cite{argamon2005measuring}.
In summary, there has been substantial work done in authorship attribution and
mainly people in forensic linguistic or computer scientists aim to build \enquote*{stylistic fingerprint of author} by using several features of a given text such as function words,
stylometry. It is a classification problem and several classifiers are used such as Na\"ive
Bayes, SVM. Among them, SVM is observed to fit best for these kinds of problems.

\section{Support Vector Machine}
Support Vector Machines (SVMs) recently gained popularity in the learning community \cite{vapnik1999overview}. In its simplest
linear form, an SVM is a hyperplane that separates
a set of positive examples from a set of negative examples with maximum interclass distance, the margin.
\autoref{fig:svm_margin} shows such a hyperplane with the associated
margin.
The formula for the output of a linear SVM is show in \autoref{svmformula}, where $w$ is the normal vector to the hyperplane, and $x$ is the input vector. The margin is defined by the distance of the hyperplane to the nearest of the positive
and negative examples.
\begin{equation}\label{svmformula}
	u = w * x + b
\end{equation}
Maximizing the margin can be
expressed as an optimization problem, as shown in  \autoref{maximizemargin}:
\begin{equation}\label{maximizemargin}
	minimize\frac{1}{2}||w||^2\ subject\ to\ y_i(w*x_i+b)\geq 1, \forall_i
\end{equation}

where $x_i$ is the $i-th$ training example and $y_i \in {-1, 1}$
is the correct output of the SVM for the $i-th$ training
example. Note that the hyperplane is only determined
by the training instances $x_i$ on the margin, \textit{the support vectors}.
Support vector machines are based on the structural
risk minimization principle from computational
learning theory \cite{vapnik1999overview}. The idea is to find a model for which
we can guarantee the lowest true error. This limits
the probability that the model will make an error on
an unseen and randomly selected test example. An
SVM finds a model which minimizes (approximately)
a bound on the true error by controlling the model
complexity (VC-Dimension). This avoids over-fitting,
which is the main problem for other semi-parametric
models.
\begin{figure}[ht]
	\centering
	\includegraphics[width=.6\textwidth, height=.6\textheight, keepaspectratio]{svm_margin}
	\caption[Support Vector Machine Hyperplane margin]{Support Vector Machine Hyperplane with the associated margin formula}
	\label{fig:svm_margin}
\end{figure}

\subsection{Support Vector Machine for authorship attribution}
Unlike currently used
classification approaches, like neural networks or decision trees, SVM allows for the processing of hundreds of thousands of features. This offers the opportunity to use all words of a text as inputs instead of a few hundred carefully selected characteristic words only. In similar text classification problems aiming at thematic categorization, the SVM has been shown to be quite effective \cite{joachims1998making}, \cite{dumais1998inductive}.
A SVM is able to classify a text with respect to content. In the framework of author attribution, it is not clear whether a specific topic addressed by the author or the structural or stylistic features of the authors language lead to a successful classification.
Among the earliest methods to be applied were various types of neural networks, typically using small sets of FWs as features \cite{holmes1995forensic}.
More recently, \citeauthor{hirst2007bigrams} used neural networks on a wide variety of features. Other studies have used k-nearest neighbor \cite{zhao2005effective}, support vector machines \cite{diederich2003authorship}, \cite{koppel2005determining}, \cite{zheng2006framework}, and Bayesian regression \cite{madigan2005author}.
Comparative studies on machine learning methods for topic-based text categorization problems \cite{dumais1998inductive}, \cite{joachims1998making} have shown that in general, support vector machine (SVM) learning is at least as good for text categorization as any other
learning method, and the same has been found for authorship attribution \cite{zheng2006framework}.
The distinctive advantage of the SVM for text categorization is its ability to process many thousand different inputs. This opens the opportunity to use all words
in a text directly as features. For each word $w_i$ the number of times of occurrence is recorded. Typically a corpus contains more than 100,000 different words,
with each text covering only a small fraction.
\citeauthor{joachims1998making} \cite{joachims1998making} used the SVM for the classification of text into different topic categories. As features he uses word stems. To establish statistically significant features he requires that each feature occurs at least three times in a text. The empirical evaluation was done
on two test collections: the Reuter-21578 news agency
data set covering different topics and the Ohsumed corpus of William Hersh describing diseases. Using about 10000 features in every case, the two SVM versions (polynomial and rbf) performed substantially better than the currently best performing conventional methods (naive Bayes, Rocchio, decision trees, k-nearest
neighbor). \citeauthor{joachims1999transductive} \cite{joachims1999transductive} used a transductive SVM for text categorization which is able to exploit the information in unlabeled training data.
\citeauthor{dumais1998inductive} \cite{dumais1998inductive} use linear SVMs for text categorization because they are both accurate and fast. They are 35 times faster to train than the next most accurate (a decision tree) of the tested classifiers. They applied SVMs to the Reuter-21578 collection, emails and web pages.
\citeauthor{drucker1999support} \cite{drucker1999support} classify emails as spam and non spam. They find that boosting trees and SVMs have similar performance in terms of accuracy and speed. SVMs train significantly faster.

\section{Studies on Reuters Corpus}
Reuters is the world’s largest international multimedia news agency, providing myriad news and mutual fund information available on \url{Reuters.com}, video, mobile, and interactive television platforms. Reuters Corpus Volume 1 (RCV1) is drawn from one of those online databases\footnote{Reuters corpora [Online]. Available, \url{http://trec.nist.gov/data/reuters/reuters.html}; 2000}. This dataset consists of all English language stories produced by Reuters journalists between August 20, 1996 and August 19, 1997. The dataset is made available on two CD-ROMs and has been formatted in XML by Reuters, Ltd. in 2000, for research purposes. Both the archiving process and later preparation of
the XML dataset involved substantial verification and validation of the content, attempts to remove spurious or duplicated documents, normalization of dateline and byline formats, addition of copyright statements, and so on \cite{cheng2011author}. The stories cover a range of content typical of a large English language international newswire. They vary from a few hundred to several thousand words in length.\\
It consists of a collection of newswire stories written in English that cover four main topics: corporate/industrial (CCAT), economics (ECAT), government/social (GCAT) and markets (MCAT).
Although it was not compiled for authorship attribution task, it has been adapted to this task in previous works. For example, in \cite{stamatatos2008author}; \cite{plakias2008tensor} the 10 most prolific authors were chosen from the CCAT category, and then, 50 examples per author for training and 50 examples for testing were selected randomly with no overlapping between training and testing sets. In further sections, we will reference to this corpus as RCV1-10.\\
In \citet{houvardas2006n}, the authors proposed another adaptation of the RCV1 corpus for the authorship attribution task. They choose the 50 most prolific authors from the Reuters Corpus, keeping 50 examples per author for training and 50 examples per author for testing with no
overlapping between them. We will refer to this corpus as RCV1-50.\\
The RCV1-10 and RCV1-50 datasets are both balanced over different authors and have their genre fixed to news.
The main category of the news in both cases is fixed to corporate/industrial, but there are many subtopics covered in the news and the length of the texts is short (from 2 to 8 KBytes). These corpora resemble a more realistic scenario, when the amount of texts is limited and the number of candidate authors is large.

\subsection{Studies on Reuters Corpus on authorship attribution}

Although, not particularly designed for evaluating author identification approaches, the RCV1
corpus contains ‘by-lines’ in many documents indicating authorship. In particular,
there are 109,433 texts with indicated authorship and 2,361 different authors in total.
RCV1 texts are short (approximately 2KBytes – 8KBytes), so they resemble a realworld author identification task where only short text samples per author may be available. Moreover, all the texts belong to the same text genre (newswire stories), so
the genre factor is reduced in distinguishing among the texts. On the other hand, there
are many duplicates (exactly the same or plagiarized texts).
The RCV1 corpus has already been used in author identification experiments. In
\citet{khmelev2003repetition} the top 50 authors (with respect to total size of articles) were selected. Moreover, in the framework of the AuthorID project, the top 114 authors of RCV1 with at least 200 available text samples were selected \cite{madigan2005author}. In contrast to these approaches, in this study, the criterion for selecting the authors was the topic of the available text
samples. Hence, the top 50 authors of texts labeled with at least one subtopic of the class CCAT (corporate/industrial) were selected. That way, it is attempted to minimize the topic factor in distinguishing among the texts. Therefore, since steps to reduce the impact of genre have been taken, it is to be hoped that authorship differences will be a more significant factor in differentiating the texts. Consequently, it is more difficult to distinguish among authors when all the text samples deal with similar topics rather than when some authors deal mainly with economics, others with foreign affairs etc.
The training corpus consists of 2,500 texts (50 per author) and the test corpus includes
other 2,500 texts (50 per author) non-overlapping with the training texts \cite{houvardas2006n}.

\section{The guardian corpus: a case of cross-topic authorship attribution}

First introduced by \citeauthor{stamatatos2013robustness} \cite{stamatatos2013robustness}, \textit{The Guardian corpus} is composed of texts published in The Guardian daily newspaper. The texts were downloaded using the publicly available API\footnote{\textit{Open Platform}, GUARDIAN, \url{http://explorer.content.guardianapis.com/}} and preprocessed to keep the unformatted main text. The majority of the corpus comprises opinion articles
(comments). The newspaper describes the opinion articles using a set of tags indicating its subject. There are eight top-level tags (World, U.S., U.K., Belief, Culture, Life\&Style, Politics, Society), each one of them having multiple subtags. It is possible (and very common) for an article to be described by multiple tags belonging to different main categories (e.g., a specific article may simultaneously belong to U.K., Politics, and Society). In order to have a clearer picture of the thematic area of the collected texts, they only used articles that belong to a single main category. Therefore, each article can be described by multiple tags, all of them belonging to a single main category. Moreover, articles coauthored by multiple authors were discarded.
In addition to opinion articles on several thematic areas, the presented corpus comprises a second text genre-book reviews. The book reviews are also described by a set of tags similar to the opinion articles. However, no thematic tag restriction was taken into account when collecting book reviews. Note that since all texts come from the same newspaper, they are expected to have been edited according to the same rules, so any significant difference among the texts is not likely to be attributed to the editing process.

\begin{table}[h!]
	\begin{center}  
		\caption[The Guardian Corpus topics distribution details]{Details about \textit{The Guardian Corpus} ("TGC") distribution in every topic among the authors.} 
		\label{tab:tableTGC}
		%\resizebox{\linewidth}{!}{  %
		\begin{tabular}{|c | c | c | c | c | c |}
			\hline 
			Author & Politics & Society & World & UK & Books \\
			\hline \hline
			CB & 12 & 4 & 11 & 14 & 16 \\ \hline
			GM & 6 & 3 & 41 & 3 & 0 \\ \hline
			HY & 8 & 6 & 35 & 5 & 3 \\ \hline
			JF & 9 & 1 & 100 & 16 & 2 \\ \hline
			MK & 7 & 0 & 36 & 3 & 2 \\ \hline
			MR & 8 & 12 & 23 & 24 & 4 \\ \hline
			NC & 30 & 2 & 9 & 7 & 5 \\ \hline
			PP & 14 & 1 & 66 & 10 & 72 \\ \hline
			PT & 17 & 36 & 12 & 5 & 4 \\ \hline
			RH & 22 & 4 & 3 & 15 & 39 \\ \hline
			SH & 100 & 5 & 5 & 6 & 2 \\ \hline
			WH & 17 & 6 & 22 & 5 & 7 \\ \hline
			ZW & 4 & 14 & 14 & 6 & 4 \\ \hline
			\textbf{Total:} & 254 & 94 & 377 & 119 & 160 \\ \hline
			%\bottomrule 
		\end{tabular} 
		%}
	\end{center}
\end{table}
\autoref{tab:tableTGC} shows details about The Guardian Corpus (“TGC”). It comprises texts from thirteen authors selected on the basis of having published texts in multiple thematic areas (Politics,
Society, World, U.K.) and different genres (opinion articles and book reviews). At most 100 texts per author and category have been collected—all of them published within a decade (from
1999 to 2009). Note that the opinion article thematic areas can be divided into two pairs of low similarity, namely Politics-Society and World-U.K. In other words, the Politics texts are more likely to have some thematic similarities with World or U.K. texts than with the Society texts.
TGC provides texts on two different genres from the same set of authors. Moreover, one genre is divided into four thematic areas. Therefore, it can be used to examine authorship
attribution models under cross-genre and cross-topic conditions  \cite{gomez2018document}.
\citeauthor{stamatatos2013robustness} (\citeyear{stamatatos2013robustness}) demonstrated that high frequency character n-grams allow to discriminate effectively between authors
not only for single-topic authorship attribution, but also for cross-topic authorship attribution. \citeauthor{sapkota2014cross} \cite{sapkota2014cross} improve the prediction results in cross-topic authorship attribution using an enriched training corpus in order to predict authors on a corpus with different topics.
The role of preprocessing steps was evaluated in \cite{markov2017improving}. The approach proposed in
that paper is considered to be more topic-neutral by their authors, because they replace
the named entities and some topic-related words while preprocessing the corpus. Their
approach showed the importance of preprocessing, because it gave the improvement
of 4\%.
In \cite{stamatatos2013robustness}, it is mentioned that the use of semantic features for the authorship attribution task usually improves the obtained results, however, very few attempts have been done
to exploit high-level features for stylometric purposes. More recently, the
usage of the distributed document representation for the cross-topic authorship attribution task has shown great results in terms of accuracy \cite{posadas2017application} and \cite{gomez2018document}, because
of its capability to encode the semantic information of texts in a low dimension vector,.
Recently, the Paragraph Vector (Doc2vec) model was proposed by Le \&
Mikolov \cite{le2014distributed} for learning distributed representation for both sentences and documents. The Doc2vec model basically treats each document as a special word and learn
both document vectors and word vectors simultaneously by predicting the target word.
Vectors obtained by the Doc2vec model outperforms both bag-of-words and word ngrams models producing the new state-of-the-art results for several text classification and sentiment analysis tasks \cite{gomez2018document}.
\textit{The Guardian corpus} offers the opportunity to explore
a scenario with different topics under the same genre with
the exception of the category “Books reviews,” which is
considered as another genre. It is assumed that each category represents a topic, which is different enough from
the other categories. In contrast to the previously described
benchmarks, The Guardian is a cross-topic, cross-genre and
unbalance benchmark, representing in this way a very challenging scenario.

\paragraph{}
At the beginning of this work, we wondered which dataset would best validate our work. In fact, we quickly realized the importance of using a dataset that did not present bias and invalidate our results. Contrary some of previous work (\cite{diederich2003authorship}, \cite{koppel2003exploiting}), we thought it was not appropriate to use datasets that could not be reproduced by other authors and on which results could not be compared with.
In the work of \citeauthor{potthast2016wrote} \cite{potthast2016wrote},
the authors performed the reimplementation of 15 authorship attribution methodologies and concluded that very few of them achieve consistent results across different corpora. In
this work, we show the consistency of our approach on 4 different datasets, evaluating a wide range of testing scenarios \cite{posadas2017application}.
Despite we had initially included datasets used in the literature such as Enron's email collection and documents collected experimentally we decided not to use them because they were not recognized as valid by other studies on authorship attribution.
We would have liked to obtain the twitter dataset used by \citeauthor{layton2010authorship} in \cite{layton2010authorship}, to have a comparison also with very short text; we wrote him and to his collaborators an email but we never got a reply :(.\\
The datasets we selected at the end of the selection process are the following:
\begin{itemize}
\item \textbf{RCV1}: both in the form RCV1\_10 and RCV1\_50, i.e. 10 and 50 authors always belonging to the CCAT category, in order to reproduce the same conditions of previous related works.
\item \textbf{GDELT}: A selection of 45 authors taken from a work of \cite{gungor2018benchmarking}, selected by BigQuery, available on the GDELT project\footnote{The GDELT Project is one of the largest publicly available digitized book database which
	has more than 3.5 million books published from 1800-2015. The GDELT Project is an
	open platform for research and analysis of global society and thus all datasets released by
	the GDELT Project are available for unlimited and unrestricted use for any academic,
	commercial, or governmental use of any kind without any fee. \url{https://www.gdeltproject.org/about.html}}.
\item \textbf{AFR}\footnote{Amazon Food Reviews available at \url{https://snap.stanford.edu/data/web-FineFoods.html}}: This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.
\end{itemize}
All previously selected datasets were used as authorship attribution closed set analysis and in single topic context.
We then also selected the TGC dataset (The Guardian Corpus) as a comparison and benchmark of our study for a cross-topic and cross-genre dataset to show the performance of the model in both single topic and cross topic contexts and understand critical points and differences between them.