\appendix
\chapter{Code}

\section{Wikipedia script}

\subsection{PetScan Query}
\label{appendix:petscanQuery}
We used as parameter depth 2 and this list of categories: \textit{Salute} (i.e. health in italian), \textit{Medicina} (i.e. medicine in italian),  \textit{Procedure mediche} (i.e. medical procedures in italian),  \textit{Diagnostica medica} (i.e. medical diagnostics in italian) and \textit{Specialit√† medica} (i.e. medical specialty in italian). The language of the result pages is by default "it" (i.e. italian).

\paragraph{Example:}
%\begin{lstlisting}[breaklines]
%\href{https://petscan.wmflabs.org/?language=it\&project=wikipedia\&depth=2\&categories=Salute\&combination=subset\&negcats=\&ns\%5B0\%5D=1\&larger=\&smaller=\&minlinks=\&maxlinks=\&before=\&after=\&max\_age=\&show\_redirects=no\&edits\%5Bbots\%5D=both\&edits\%5Banons\%5D=both\&edits\%5Bflagged\%5D=both\&page\_image=any\&ores\_type=any\&ores\_prob\_from=\&ores\_prob\_to=\&ores\_prediction=any\&templates\_yes=\&templates\_any=\&templates\_no=\&outlinks\_yes=\&outlinks\_any=\&outlinks\_no=\&links\_to\_all=\&links\_to\_any=\&links\_to\_no=\&sparql=\&manual\_list=\&manual\_list\_wiki=\&pagepile=\&wikidata\_source\_sites=\&subpage\_filter=either\&common\_wiki=auto\&source\_combination=\&wikidata\_item=no\&wikidata\_label\_language=\&wikidata\_prop\_item\_use=\&wpiu=any\&sitelinks\_yes=\&sitelinks\_any=\&sitelinks\_no=\&min\_sitelink\_count=\&max\_sitelink\_count=\&labels\_yes=\&cb\_labels\_yes\_l=1\&langs\_labels\_yes=\&labels\_any=\&cb\_labels\_any\_l=1\&langs\_labels\_any=\&labels\_no=\&cb\_labels\_no\_l=1\&langs\_labels\_no=\&format=json\&output\_compatability=catscan\&sortby=none\&sortorder=ascending\&regexp\_filter=\&min\_redlink\_count=1\&doit=Do\%20it\%21\&interface\_language=en\&active\_tab=tab\_output}{PetScan query depth=2, category=Salute and language=it} \\
\textit{PetScan query \textnormal{depth=}2, \textnormal{category=}Salute and \textnormal{language=}it} \\ \\
\url{https://petscan.wmflabs.org/?language=it\&project=wikipedia\&depth=2\&categories=Salute\&combination=subset\&negcats=\&ns\%5B0\%5D=1\&larger=\&smaller=\&minlinks=\&maxlinks=\&before=\&after=\&max\_age=\&show\_redirects=no\&edits\%5Bbots\%5D=both\&edits\%5Banons\%5D=both\&edits\%5Bflagged\%5D=both\&page\_image=any\&ores\_type=any\&ores\_prob\_from=\&ores\_prob\_to=\&ores\_prediction=any\&templates\_yes=\&templates\_any=\&templates\_no=\&outlinks\_yes=\&outlinks\_any=\&outlinks\_no=\&links\_to\_all=\&links\_to\_any=\&links\_to\_no=\&sparql=\&manual\_list=\&manual\_list\_wiki=\&pagepile=\&wikidata\_source\_sites=\&subpage\_filter=either\&common\_wiki=auto\&source\_combination=\&wikidata\_item=no\&wikidata\_label\_language=\&wikidata\_prop\_item\_use=\&wpiu=any\&sitelinks\_yes=\&sitelinks\_any=\&sitelinks\_no=\&min\_sitelink\_count=\&max\_sitelink\_count=\&labels\_yes=\&cb\_labels\_yes\_l=1\&langs\_labels\_yes=\&labels\_any=\&cb\_labels\_any\_l=1\&langs\_labels\_any=\&labels\_no=\&cb\_labels\_no\_l=1\&langs\_labels\_no=\&format=json\&output\_compatability=catscan\&sortby=none\&sortorder=ascending\&regexp\_filter=\&min\_redlink\_count=1\&doit=Do\%20it\%21\&interface\_language=en\&active\_tab=tab\_output}
%\end{lstlisting}

\subsection{MLStripper}
\label{appendix:MLStripper}
To retrieve a single document, we parsed the HTML of every Wikipedia page with HTMLParser subclassed with a customized class MLStripper, that stripped away all HTML tags.
\begin{lstlisting}[language=Python]
class MLStripper(HTMLParser):
    def __init__(self):
        super().__init__()
        self.reset()
        self.fed = []

    def handle_data(self, d):
        self.fed.append(d)

    def get_data(self):
        return ''.join(self.fed)

def strip_tags(html):
    s = MLStripper()
    s.feed(html)
    return s.get_data()

\end{lstlisting}

\section{Word2vec}

\subsection{Load pre-trained vectors}
\label{appendix:loadWikiW2V}
Code \ref{code:loadWikiW2V} shows how to load the model from the pre-trained vectors.
\begin{lstlisting}[caption={How to load Wikipedia pre-trained model}, label={code:loadWikiW2V}, language=Python, breaklines=true, frame=single] 
# word2vec model
from gensim.models import Word2Vec
model = Word2Vec.load('wiki_iter=5_algorithm=skipgram_window=10_size=300_neg-samples=10.m')
\end{lstlisting}

\subsection{Train model}
\label{appendix:trainModel}
\begin{minipage}{\linewidth}
Code \ref{code:trainModel} shows how to build the vocabulary for the model and train it.
\begin{lstlisting}[caption={How to build vocabulary and train model}, label={code:trainModel}, language=Python, breaklines=true, frame=single] 
# build vocabulary and train model
model = gensim.models.Word2Vec(documents,
                             size=200,
                             window=10,
                             min_count=2,
                             workers=10)
model.train(documents, 
            total_examples=len(documents),
            epochs=10)
\end{lstlisting}
\end{minipage}

\subsection{Plotting}
\label{appendix:plotModel}
Code \ref{code:plotModel} shows how to plot words' vectors. We used pyplot from \textit{matplotlib}\footnote{\url{https://matplotlib.org/api/pyplot_api.html}} module and PCA from \textit{sklearn.decomposition}\footnote{\url{http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html}} module.
\begin{lstlisting}[caption={How to plot words' vectors}, label={code:plotModel}, language=Python, breaklines=true, frame=single] 
from matplotlib import pyplot
from sklearn.decomposition import PCA
def plot(model, words):
	X = model[model.wv.vocab]
	pca = PCA(n_components=2)
	result = pca.fit_transform(X)
	pyplot.scatter(result[:, 0], result[:, 1])
	for i, word in enumerate(words):
		pyplot.annotate((word[0], float(round(word[1], 2))), xy=(result[i, 0], result[i, 1]))
	pyplot.show()
\end{lstlisting}


\listoffigures
\listoftables
%\listoffigures
%\listoftables