{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[FINAL]: SVM_AA_RCV1.ipynb","provenance":[{"file_id":"1PBcUEEmV8bZ8IxxehSe4LV304CUyybIo","timestamp":1609688619590}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lj6IyOVHHMfU"},"source":["# Gabriele Calarota\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8IXYxuUOKGD","executionInfo":{"status":"ok","timestamp":1611606377957,"user_tz":-60,"elapsed":45508,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"5dfdf01e-294b-46ef-a697-eb9187510d4e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YE7XIOj-1-_u"},"source":["NUM_AUTHORS = 10\n","N_DOCS = 100\n","N_DOCS_TEST_SET = 0\n","NORMALIZE_WORDS_IN_DOCS = None\n","N_THRESHOLD = None\n","USE_BOW=False\n","USE_TFIDF=True\n","USE_W2V=False\n","PROJECT_NAME = \"RCV1\"\n","DATASET_FILENAME = 'Reuteurs/RCV1/rcv1_ccat_parsed_renamed.csv'\n","USE_TEXT_DISTORTION = False\n","K_text_distortion = 10000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A87lOJwwQSlm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606404712,"user_tz":-60,"elapsed":72251,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"a2b34dea-ccdf-47ad-f497-640d23656db5"},"source":["!pip install -q tpot"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 92kB 4.4MB/s \n","\u001b[K     |████████████████████████████████| 157.5MB 91kB/s \n","\u001b[K     |████████████████████████████████| 163kB 51.1MB/s \n","\u001b[?25h  Building wheel for stopit (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6TtA1PaKHMfa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606411454,"user_tz":-60,"elapsed":78990,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"9ca00870-a21c-4cd8-fd7c-09455bae5c77"},"source":["import os\n","import re\n","from io import StringIO\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import tensorflow as tf\n","import xgboost as xgb\n","%matplotlib inline\n","\n","from sklearn.svm import SVC\n","from keras.models import Sequential\n","from keras.layers.recurrent import LSTM, GRU\n","from keras.layers.core import Dense, Activation, Dropout\n","from keras.layers.embeddings import Embedding\n","from keras.layers.normalization import BatchNormalization\n","from keras.utils import np_utils, to_categorical\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.callbacks import EarlyStopping\n","from sklearn import preprocessing, decomposition, model_selection, metrics\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n","from sklearn.decomposition import KernelPCA, PCA\n","from keras.layers import GlobalAvgPool1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D, AveragePooling1D\n","from keras.preprocessing import sequence, text\n","from keras.callbacks import EarlyStopping\n","from keras import backend as K\n","import nltk\n","from nltk import word_tokenize\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.corpus import wordnet as wn\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","import tarfile\n","import zipfile\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_selection import SelectFwe, f_classif\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import make_pipeline, make_union\n","from sklearn.svm import LinearSVC\n","from tpot.builtins import StackingEstimator\n","from tpot.export_utils import set_param_recursive\n","from sklearn.preprocessing import FunctionTransformer\n","from copy import copy\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, classification_report"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"S5K2mKLYASax","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606412932,"user_tz":-60,"elapsed":80452,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"01ee5468-ac06-45d0-9a3b-40071e126be9"},"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"aaMGpSC6HMfh"},"source":["base_dir = '/content/drive/Shared drives/Tesi_AuthorshipAttribution_Calarota/Dataset/'\n","#base_dir = \"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gplM6Vwj2C7d"},"source":["def print_stats_dataset(dataframe):\n","  num_text_total = len(dataframe)\n","  df_group_by_author = dataframe.groupby('author')\n","  df_count = df_group_by_author['articles'].count()\n","  num_of_authors = df_group_by_author.size().reset_index(name='counts').count()\n","  num_text_per_author_mean=df_group_by_author['articles'].count().mean()\n","  text_length_per_author=df_group_by_author['articles'].apply(lambda x: np.mean(x.str.len())).reset_index(name='mean_len_text')\n","  # dataframe['number_of_words'] = dataframe['articles'].apply(lambda x: len([word.lower() for sent in nltk.sent_tokenize(x) for word in nltk.word_tokenize(sent)]))\n","  # print(f\"Total words: {dataframe['number_of_words'].sum()}. Mean per article: {dataframe['number_of_words'].mean()}\")\n","  print(f\"Numero di testi totale: {num_text_total}\")\n","  print(f\"Numero di testi per autore in media: {num_text_per_author_mean}\")\n","  print(f\"Numero di autori: {num_of_authors['author']}\")\n","  print(f\"Lunghezza media testo per autore in media: {text_length_per_author['mean_len_text'].mean()}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0CNDVlH8LMk","colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"status":"ok","timestamp":1611606475454,"user_tz":-60,"elapsed":897,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"09af1e57-a770-4d21-cffc-cf469f6a68bc"},"source":["dataset = pd.read_csv(os.path.join(base_dir, DATASET_FILENAME)) \n","dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>articles</th>\n","      <th>author</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Chrysler Corp. Tuesday announced $380 million ...</td>\n","      <td>David Lawder</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>CompuServe Corp. Tuesday reported a surprising...</td>\n","      <td>Susan Zimmerman</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>CompuServe Corp. Tuesday reported a surprising...</td>\n","      <td>Susan Zimmerman</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7</td>\n","      <td>Sprint Corp. Tuesday announced plans to offer ...</td>\n","      <td>Susan Nadeau</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9</td>\n","      <td>Kansas and Arizona filed lawsuits against some...</td>\n","      <td>Greg Frost</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...           author\n","0           1  ...     David Lawder\n","1           2  ...  Susan Zimmerman\n","2           3  ...  Susan Zimmerman\n","3           7  ...     Susan Nadeau\n","4           9  ...       Greg Frost\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"eTKTMVtN2Wan","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606475188,"user_tz":-60,"elapsed":613,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"763800bb-f7aa-444f-8825-adfc1791824f"},"source":["print_stats_dataset(dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Numero di testi totale: 35667\n","Numero di testi per autore in media: 23.403543307086615\n","Numero di autori: 1524\n","Lunghezza media testo per autore in media: 3056.149551256315\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gsEiRZO1Jfu8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606475190,"user_tz":-60,"elapsed":598,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"32caa95f-1bbc-4d41-f529-cfd27823fe97"},"source":["def get_top_ten_authors(dataframe, number_prune=10):\n","  df_group_by_author = dataframe.groupby('author')\n","  df_count = df_group_by_author['articles'].count()\n","  num_of_authors = df_group_by_author.size().reset_index(name='counts')\n","  sorted_authors = num_of_authors.sort_values(by='counts', ascending=False)\n","  id_of_author = sorted_authors['author'].to_list()[:number_prune]\n","  return id_of_author\n","\n","list_of_top_ten_authors = get_top_ten_authors(dataset, number_prune=NUM_AUTHORS)\n","dataset = dataset[dataset.author.isin(list_of_top_ten_authors)]\n","print_stats_dataset(dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Numero di testi totale: 2644\n","Numero di testi per autore in media: 264.4\n","Numero di autori: 10\n","Lunghezza media testo per autore in media: 3126.2182190049957\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zMys1uLqNI9-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606475192,"user_tz":-60,"elapsed":581,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"316c965d-6e4b-4c19-baa9-dc583535cdbd"},"source":["def get_only_n_docs_for_authors(dataframe, n_docs=200, threshold_document_length=600):\n","  if threshold_document_length is not None:\n","    dataframe = dataframe[dataframe.articles.str.len() > threshold_document_length]\n","  if n_docs is not None:\n","    dataframe = dataframe.groupby('author').head(n_docs).reset_index(drop=True)\n","  return dataframe\n","dataset = get_only_n_docs_for_authors(dataset, n_docs=N_DOCS+N_DOCS_TEST_SET, threshold_document_length=N_THRESHOLD)\n","print_stats_dataset(dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Numero di testi totale: 1000\n","Numero di testi per autore in media: 100.0\n","Numero di autori: 10\n","Lunghezza media testo per autore in media: 3093.821\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"42V08q17HMfw","colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"status":"ok","timestamp":1611606475548,"user_tz":-60,"elapsed":912,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"a231d6f4-fe65-46c1-df06-d8e2d9e460d0"},"source":["dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>articles</th>\n","      <th>author</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Chrysler Corp. Tuesday announced $380 million ...</td>\n","      <td>David Lawder</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>152</td>\n","      <td>Huntsman Corp. on Tuesday dropped its $460 mil...</td>\n","      <td>Robin Sidel</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1171</td>\n","      <td>Qantas Airways Ltd has begun a regular air car...</td>\n","      <td>Jim Gilchrist</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1172</td>\n","      <td>Qantas Airways regional general manager freigh...</td>\n","      <td>Jim Gilchrist</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1195</td>\n","      <td>Acting Indian commissioner in Hong Kong, Dipak...</td>\n","      <td>Jim Gilchrist</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                           articles         author\n","0           1  Chrysler Corp. Tuesday announced $380 million ...   David Lawder\n","1         152  Huntsman Corp. on Tuesday dropped its $460 mil...    Robin Sidel\n","2        1171  Qantas Airways Ltd has begun a regular air car...  Jim Gilchrist\n","3        1172  Qantas Airways regional general manager freigh...  Jim Gilchrist\n","4        1195  Acting Indian commissioner in Hong Kong, Dipak...  Jim Gilchrist"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"ihZiaV_jHMf6","colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"status":"ok","timestamp":1611606475558,"user_tz":-60,"elapsed":895,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"6264fc45-2516-446e-e662-de6462bdd1cd"},"source":["dataset['author'].value_counts().plot()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7ffa100f42b0>"]},"metadata":{"tags":[]},"execution_count":0},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"SF0wrxAOHMgD"},"source":["Train and test data are similarly distributed. An article can be attributed to an author based on the topic and content of the article or the author writing style or mix of both. In my basic approach, I will try to solve the problem by leveraging the frequency of words in the article, which represents the topic of an article. For this, I will construct a TF-IDF matrix. I am not going to rely on the default tokenizer provided by the scikit learn; I will create one for myself. The custom tokenizer involved three steps:\n","\n","* Tokenize the article into sentences and sentences into words\n","* Filter the tokens with smaller lengths (assuming smaller words doesn't really say anything about the topic), whether a word is stop word or not, and whether the word is present in the dictionary or not\n","* Stem the words \n","\n","I am also going to construct a raw counts matrix as some models like MultinomialNB often perform better on raw counts"]},{"cell_type":"code","metadata":{"id":"Glxvw26eLKY8"},"source":["def get_wk_bnc(k=2000):\n","  bnc_df = pd.read_csv(os.path.join(base_dir, 'bnc_lemma_parsed.csv'))\n","  select_df = bnc_df.head(k)\n","  return list(select_df['word'].values)\n","\n","if USE_TEXT_DISTORTION:\n","  WK = get_wk_bnc(k=K_text_distortion)\n","  print(len(WK))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NZ73widCHMgE"},"source":["def tokenize_and_stem(text):\n","    \"\"\"\n","    Below function tokenizes and lemmatizes the texts. It also does some cleaning by removing non dictionary words\n","    This can be used to replace default tokenizer provided by feature extraction api of sklearn.\n","    :param text: str\n","    :return: list\n","    \"\"\"\n","    stemmer = SnowballStemmer(\"english\")\n","    stop_words = stopwords.words(\"english\")\n","    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n","    filtered_tokens = []\n","    for token in tokens:\n","        if re.search(r'[a-zA-Z-]{4,}', token) and token not in stop_words and len(wn.synsets(token)) > 0:\n","            token.strip()\n","            filtered_tokens.append(token)\n","    filtered_tokens = [stemmer.stem(token) for token in filtered_tokens]\n","    return filtered_tokens\n","  \n","def simple_tokenizer(text):\n","    text = re.sub('\"([^\"]*)\"', '', text)\n","    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n","    filtered_tokens = []\n","    for token in tokens:\n","        if len(wn.synsets(token)) > 0:\n","            token.strip()\n","            filtered_tokens.append(token)\n","    return filtered_tokens\n","\n","def only_remove_quoting_tokenizer(text):\n","  text = re.sub('\"([^\"]*)\"', '', text)\n","  tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n","  return tokens\n","\n","def only_remove_quoting_tokenizer_with_threshold(text):\n","  text = re.sub('\"([^\"]{1,})\"', '', text)\n","  tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n","  return tokens\n","\n","\n","def text_distortion_tokenizer_DV_MA(text):\n","  text = re.sub('\"([^\"]*)\"', '', text)\n","  tokens = [word.lower().strip() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n","  filtered_tokens = []\n","  # implementing DV_MA\n","  for token in tokens:\n","    if token.lower() not in WK:\n","      # replacing all digits in token with #\n","      token = re.sub('\\d', '#', token)\n","      # replacing each letter in t with *\n","      token = re.sub('[a-zA-Z]', '*', token)\n","      filtered_tokens.append(token)\n","\n","  return filtered_tokens\n","\n","def text_distortion_tokenizer_DV_SA(text):\n","  text = re.sub('\"([^\"]*)\"', '', text)\n","  tokens = [word.lower().strip() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n","  filtered_tokens = []\n","  # implementing DV_MA\n","  for token in tokens:\n","    if token.lower() not in WK:\n","      # replacing all digits in token with #\n","      token = re.sub(\"\\d+\", \"#\", token)\n","      # replacing each letter in t with *\n","      token = re.sub('[a-zA-Z]+', '*', token)\n","      filtered_tokens.append(token)\n","\n","  return filtered_tokens\n","\n","# custom_tokenizer = tokenize_and_stem\n","# custom_tokenizer = text_distortion_tokenizer_DV_MA\n","# custom_tokenizer = text_distortion_tokenizer_DV_SA\n","# custom_tokenizer = simple_tokenizer\n","# custom_tokenizer = None\n","# custom_tokenizer = only_remove_quoting_tokenizer\n","custom_tokenizer = only_remove_quoting_tokenizer_with_threshold"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r1lL6lyzHMgH"},"source":["tfidf_vec = TfidfVectorizer(max_df=0.75, max_features=None,\n","                            min_df=0.02, use_idf=False, tokenizer=custom_tokenizer, ngram_range=(1, 4))\n","counter_vect = CountVectorizer(max_df=0.8, max_features=10000,\n","                               min_df=0.02, tokenizer=custom_tokenizer, ngram_range=(1, 2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRAv0GkqiRcd"},"source":["# df_clean_test_set = dataset.groupby('author').head(N_DOCS_TEST_SET).reset_index(drop=True)\n","# df_clean_test_set.head()\n","# print_stats_dataset(df_clean_test_set)\n","# dataset = pd.concat([dataset,df_clean_test_set]).drop_duplicates(keep=False)\n","# print_stats_dataset(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VbZLHuu1C0dn"},"source":["if NORMALIZE_WORDS_IN_DOCS:\n","  # dataset['articles'] = dataset['articles'].apply(lambda x: \" \".join([word.lower() for sent in nltk.sent_tokenize(x) for word in nltk.word_tokenize(sent)][:NORMALIZE_WORDS_IN_DOCS]))\n","  dataset['articles'] = dataset['articles'].apply(lambda x: \" \".join(x.split(' ')[:NORMALIZE_WORDS_IN_DOCS]))\n","  print_stats_dataset(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Da5JEkD3ktg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606475194,"user_tz":-60,"elapsed":448,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"cbbfcf6d-2cfc-482d-f4aa-fd3794cbf46c"},"source":["# Even split 50 & 50 per author and document\n","# df_train, df_test = train_test_split(dataset, test_size=0.2, random_state=0)\n","df_train = dataset.groupby('author').head(N_DOCS/2).reset_index(drop=True)\n","df_train.head()\n","print_stats_dataset(df_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Numero di testi totale: 500\n","Numero di testi per autore in media: 50.0\n","Numero di autori: 10\n","Lunghezza media testo per autore in media: 3070.9739999999997\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FxoDNSqj4GMe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606475196,"user_tz":-60,"elapsed":437,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"0558a12c-03f5-4509-9430-72a25a406b1c"},"source":["# get difference between dataset and df_train for df_test\n","df_test = pd.concat([dataset,df_train]).drop_duplicates(keep=False)\n","df_test.head()\n","print_stats_dataset(df_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Numero di testi totale: 500\n","Numero di testi per autore in media: 50.0\n","Numero di autori: 10\n","Lunghezza media testo per autore in media: 3116.6679999999997\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3glTRRkK-EQx"},"source":["## => Extracting features"]},{"cell_type":"code","metadata":{"id":"_jxdXsN0HMgS"},"source":["def tfidf_fit_transform():\n","  # df_train, df_test = train_test_split(dataset, test_size=0.5, random_state=0)\n","  tfidf_train = tfidf_vec.fit_transform(df_train['articles'])\n","  tfidf_test = tfidf_vec.transform(df_test['articles'])\n","  return tfidf_train, tfidf_test\n","\n","def counter_fit_transform():\n","  counter_train = counter_vect.fit_transform(df_train['articles'])\n","  counter_test = counter_vect.transform(df_test['articles'])\n","  return counter_train, counter_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2hZb1kAjQhOl"},"source":["le = LabelEncoder()\n","df_train['target'] = le.fit_transform(df_train['author'])\n","# df_test['target'] = le.fit_transform(df_test['author'])\n","#df_test['author'] = df_test['author'].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n","# le.classes_ = np.append(le.classes_, '<unknown>')\n","df_test['target']  = le.transform(df_test['author'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5-oMu4JOBzM0"},"source":["\n","While above models tried to use the classify the articles based on the words and their frequencies, I will try to build a sequence model that tries to capture the writing style of an author. However, I am dubious about the effectiveness of these models considering the limited amount of data.\n","\n","I will change the tokenizer by removing stem as I am going to replace words with Glove embeddings that provide relevant words vectors to all verb forms of a word. Below are the changes that I will make:\n","\n","Remove words in the quotes as they don't contribute to capture the writing style of the author.\n","Not stemming the words to replace the words with corresponsing Glove vectors\n","Not removing stop words, as some authors whose articles aren't published online may not hesitate to use lot of stop words\n","\n"]},{"cell_type":"code","metadata":{"id":"_ApcsA7_wH9u"},"source":["def build_custom_w2v_model():\n","  import numpy as np\n","  import keras.backend as K\n","  from keras.models import Sequential\n","  from keras.layers import Dense, Embedding, Lambda\n","  from keras.utils import np_utils\n","  from keras.preprocessing import sequence\n","  from keras.preprocessing.text import Tokenizer\n","  import gensim\n","  vectorize = Tokenizer()\n","  vectorize.fit_on_texts(dataset['articles'])\n","  data = vectorize.texts_to_sequences(dataset['articles'])\n","  total_vocab = sum(len(s) for s in data)\n","  word_count = len(vectorize.word_index) + 1\n","  window_size = 2\n","  print(f\"total vocab: {total_vocab}\")\n","  print(f\"word count: {word_count}\")\n","\n","  def cbow_model(data, window_size, total_vocab):\n","      total_length = window_size*2\n","      for text in data:\n","          text_len = len(text)\n","          for idx, word in enumerate(text):\n","              context_word = []\n","              target   = []            \n","              begin = idx - window_size\n","              end = idx + window_size + 1\n","              context_word.append([text[i] for i in range(begin, end) if 0 <= i < text_len and i != idx])\n","              target.append(word)\n","              contextual = sequence.pad_sequences(context_word, maxlen=total_length)\n","              final_target = np_utils.to_categorical(target, total_vocab)\n","              yield(contextual, final_target) \n","\n","  model = Sequential()\n","  model.add(Embedding(input_dim=total_vocab, output_dim=100, input_length=window_size*2))\n","  model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,)))\n","  model.add(Dense(total_vocab, activation='softmax'))\n","  model.compile(loss='categorical_crossentropy', optimizer='adam')\n","  for i in range(10):\n","      cost = 0\n","      for contextual, final_target in cbow_model(data, window_size, total_vocab):\n","          cost += model.train_on_batch(contextual, final_target)\n","      print(i, cost)\n","  dimensions=100\n","  vect_file = open(os.path.join(os.path.dirname(os.path.join(base_dir, DATASET_FILENAME)), 'vectors.txt') ,'w')\n","  vect_file.write('{} {}\\n'.format(total_vocab,dimensions))\n","  weights = model.get_weights()[0]\n","  for text, i in vectorize.word_index.items():\n","      final_vec = ' '.join(map(str, list(weights[i, :])))\n","      vect_file.write('{} {}\\n'.format(text, final_vec))\n","  vect_file.close()\n","\n","if USE_W2V and not os.path.exists(os.path.join(os.path.dirname(os.path.join(base_dir, DATASET_FILENAME)), 'vectors.txt')):\n","  build_custom_w2v_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uEyodaoNCdka"},"source":["from gensim.scripts.glove2word2vec import glove2word2vec\n","from gensim.models import KeyedVectors\n","\n","class Word2VecVectorizer:\n","  def __init__(self, model):\n","    print(\"Loading in word vectors...\")\n","    self.word_vectors = model\n","    print(\"Finished loading in word vectors\")\n","\n","  def fit(self, data):\n","    pass\n","\n","  def transform(self, data):\n","    # determine the dimensionality of vectors\n","    v = self.word_vectors.get_vector('king')\n","    self.D = v.shape[0]\n","\n","    X = np.zeros((len(data), self.D))\n","    n = 0\n","    emptycount = 0\n","    for sentence in data:\n","      tokens = sentence.split()\n","      vecs = []\n","      m = 0\n","      for word in tokens:\n","        try:\n","          # throws KeyError if word not found\n","          vec = self.word_vectors.get_vector(word)\n","          vecs.append(vec)\n","          m += 1\n","        except KeyError:\n","          pass\n","      if len(vecs) > 0:\n","        vecs = np.array(vecs)\n","        X[n] = vecs.mean(axis=0)\n","      else:\n","        emptycount += 1\n","      n += 1\n","    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n","    return X\n","\n","\n","  def fit_transform(self, data):\n","    self.fit(data)\n","    return self.transform(data)\n","\n","def w2v_fit_transform():\n","  USE_GLOVE = False\n","  if USE_GLOVE:\n","    glove_path = os.path.join(base_dir, 'glove.6B.50d.txt')\n","    word2vec_output_file = glove_path+'.word2vec'\n","    if not os.path.exists(word2vec_output_file):\n","      glove2word2vec(glove_path, word2vec_output_file)\n","  else:\n","    word2vec_output_file = os.path.join(os.path.dirname(\n","        os.path.join(base_dir, DATASET_FILENAME)), 'vectors.txt')\n","  model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n","\n","  # Set a word vectorizer\n","  vectorizer = Word2VecVectorizer(model)\n","  # Get the sentence embeddings for the train dataset\n","  w2v_train = vectorizer.fit_transform(df_train['articles'])\n","  # Get the sentence embeddings for the test dataset\n","  w2v_test = vectorizer.transform(df_test['articles'])\n","  print(w2v_train.shape,w2v_test.shape)\n","  return w2v_train, w2v_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IEOpksVCCXsH"},"source":["def teapot_search():\n","  from tpot import TPOTClassifier, TPOTRegressor\n","  #tpot_settings = dict(verbosity=2, random_state = 1234, scoring = 'accuracy', warm_start = True, config_dict='TPOT sparse')\n","  #auto_reg = TPOTRegressor(generations=2, population_size=5, **tpot_settings)\n","  #auto_reg.fit(tfidf_train, df_train['target'])\n","  #print(auto_reg.score(tfidf_test, df_test['target']))\n","  #auto_reg.export('tpot_exported_pipeline.py')\n","  #pipeline_optimizer = TPOTClassifier()\n","  pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5,\n","                                      random_state=42, verbosity=2, scoring='accuracy', config_dict='TPOT sparse')\n","  pipeline_optimizer.fit(tfidf_train, df_train['target'])\n","  print(pipeline_optimizer.score(tfidf_test, df_test['target']))\n","  pipeline_optimizer.export('tpot_exported_pipeline.py')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T2dZMihcDrTg"},"source":["def use_features(tfidf=False, bow=False, w2v=False):\n","  if tfidf:\n","    return tfidf_fit_transform()\n","  elif bow:\n","    return counter_fit_transform()\n","  elif w2v:\n","    return w2v_fit_transform()\n","  else:\n","    return tfidf_train, tfidf_test\n","\n","training_features, testing_features = use_features(tfidf=USE_TFIDF, bow=USE_BOW, w2v=USE_W2V)\n","training_target = df_train['target']\n","testing_target = df_test['target']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gNCUVg8kDfuk"},"source":["def double_pipeline():\n","  # Average CV score on the training set was: 0.9173333333333333\n","  exported_pipeline = make_pipeline(\n","      make_union(\n","          FunctionTransformer(copy),\n","          SelectFwe(score_func=f_classif, alpha=0.004)\n","      ),\n","      LinearSVC(C=10.0, dual=True, loss=\"squared_hinge\", penalty=\"l2\", tol=0.000001, max_iter=10)\n","  )\n","  # Fix random state for all the steps in exported pipeline\n","  set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n","  return exported_pipeline\n","\n","def single_pipeline():\n","  # Average CV score on the training set was: 0.6912\n","  exported_pipeline = LinearSVC(C=20.0, dual=True, loss=\"hinge\", penalty=\"l2\", tol=0.0001)\n","  # Fix random state in exported estimator\n","  if hasattr(exported_pipeline, 'random_state'):\n","      setattr(exported_pipeline, 'random_state', 42)\n","  return exported_pipeline\n","\n","def onevsrestclassifier():\n","  from sklearn.multiclass import OneVsRestClassifier\n","  return OneVsRestClassifier(LinearSVC(C=10.0, dual=True, loss=\"squared_hinge\", penalty=\"l2\", tol=0.000001, multi_class='ovr', random_state=42, max_iter=10))\n","\n","def linearsdg():\n","  from sklearn.linear_model import SGDClassifier\n","  return SGDClassifier(alpha=0.00001, penalty='elasticnet', max_iter=50, random_state=42)\n","\n","exported_pipeline = double_pipeline()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RnKqkobCDm-w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606475197,"user_tz":-60,"elapsed":356,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"305d1cb0-5839-4b16-ae76-8bb54233495c"},"source":["exported_pipeline.fit(training_features, training_target)\n","predicted = exported_pipeline.predict(testing_features)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"PkPPT3IdGbOb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606475273,"user_tz":-60,"elapsed":419,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"811f295b-27a1-4d8f-95f6-2679ba17673f"},"source":["accuracy_result = accuracy_score(testing_target, predicted)\n","precision_result = precision_score(testing_target, predicted, average='macro')\n","recall_result = recall_score(testing_target, predicted, average='macro')\n","f1_result = f1_score(testing_target, predicted, average='macro')\n","print(f\"Accuracy: {accuracy_result}\\nPrecision: {precision_result}\\nRecall: {recall_result}\\nF1_macro: {f1_result}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.922\n","Precision: 0.9239493393212076\n","Recall: 0.922\n","F1_macro: 0.9209225933426136\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2iOQdfLpHRrx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606475276,"user_tz":-60,"elapsed":406,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"eeff33c6-f931-4113-d46a-a7560777e717"},"source":["print(classification_report(testing_target, predicted))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.83      0.88      0.85        50\n","           1       0.91      1.00      0.95        50\n","           2       0.85      0.92      0.88        50\n","           3       0.87      0.82      0.85        50\n","           4       0.98      1.00      0.99        50\n","           5       1.00      0.94      0.97        50\n","           6       1.00      0.92      0.96        50\n","           7       0.91      1.00      0.95        50\n","           8       0.96      1.00      0.98        50\n","           9       0.93      0.74      0.82        50\n","\n","    accuracy                           0.92       500\n","   macro avg       0.92      0.92      0.92       500\n","weighted avg       0.92      0.92      0.92       500\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M4nEjar4t0oZ"},"source":["## REPEATED CROSS VALIDATION"]},{"cell_type":"code","metadata":{"id":"svwdA9cHt4_h"},"source":["# evaluate a logistic regression model using repeated k-fold cross-validation\n","from numpy import mean\n","from numpy import std\n","from sklearn.model_selection import RepeatedKFold\n","from sklearn.model_selection import cross_val_score\n","\n","tfidf_dataset = tfidf_vec.fit_transform(dataset['articles'])\n","labels_dataset = le.fit_transform(dataset['author'])\n","\n","X = tfidf_dataset\n","y = labels_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Udaip4sfdPEt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606475278,"user_tz":-60,"elapsed":383,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"41ef5fb9-cf95-424c-a337-8bd6f35b1e4e"},"source":["from sklearn.model_selection import StratifiedKFold, KFold, StratifiedShuffleSplit\n","import numpy as np\n","\n","skf = StratifiedKFold(n_splits=5)\n","for train, test in skf.split(X, y):\n","  print('train -  {}   |   test -  {}'.format(\n","      np.bincount(y[train]), np.bincount(y[test])))\n","\n","\n","# evaluate model\n","scores = cross_val_score(exported_pipeline, X, y, scoring='accuracy', cv=skf, n_jobs=-1)\n","\n","# report performance\n","print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train -  [80 80 80 80 80 80 80 80 80 80]   |   test -  [20 20 20 20 20 20 20 20 20 20]\n","train -  [80 80 80 80 80 80 80 80 80 80]   |   test -  [20 20 20 20 20 20 20 20 20 20]\n","train -  [80 80 80 80 80 80 80 80 80 80]   |   test -  [20 20 20 20 20 20 20 20 20 20]\n","train -  [80 80 80 80 80 80 80 80 80 80]   |   test -  [20 20 20 20 20 20 20 20 20 20]\n","train -  [80 80 80 80 80 80 80 80 80 80]   |   test -  [20 20 20 20 20 20 20 20 20 20]\n","Accuracy: 0.920 (0.025)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1Y-ddWQteUVE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606475280,"user_tz":-60,"elapsed":371,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"9b7df951-dd55-4796-c712-a60d4d1c7354"},"source":["from sklearn.model_selection import StratifiedKFold, KFold, StratifiedShuffleSplit\n","import numpy as np\n","\n","skf = StratifiedShuffleSplit(n_splits=10, test_size=0.5, random_state=42)\n","for train, test in skf.split(X, y):\n","  print('train -  {}   |   test -  {}'.format(\n","      np.bincount(y[train]), np.bincount(y[test])))\n","\n","\n","# evaluate model\n","scores = cross_val_score(exported_pipeline, X, y, scoring='accuracy', cv=skf, n_jobs=-1)\n","\n","# report performance\n","print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train -  [50 50 50 50 50 50 50 50 50 50]   |   test -  [50 50 50 50 50 50 50 50 50 50]\n","train -  [50 50 50 50 50 50 50 50 50 50]   |   test -  [50 50 50 50 50 50 50 50 50 50]\n","train -  [50 50 50 50 50 50 50 50 50 50]   |   test -  [50 50 50 50 50 50 50 50 50 50]\n","train -  [50 50 50 50 50 50 50 50 50 50]   |   test -  [50 50 50 50 50 50 50 50 50 50]\n","train -  [50 50 50 50 50 50 50 50 50 50]   |   test -  [50 50 50 50 50 50 50 50 50 50]\n","train -  [50 50 50 50 50 50 50 50 50 50]   |   test -  [50 50 50 50 50 50 50 50 50 50]\n","train -  [50 50 50 50 50 50 50 50 50 50]   |   test -  [50 50 50 50 50 50 50 50 50 50]\n","train -  [50 50 50 50 50 50 50 50 50 50]   |   test -  [50 50 50 50 50 50 50 50 50 50]\n","train -  [50 50 50 50 50 50 50 50 50 50]   |   test -  [50 50 50 50 50 50 50 50 50 50]\n","train -  [50 50 50 50 50 50 50 50 50 50]   |   test -  [50 50 50 50 50 50 50 50 50 50]\n","Accuracy: 0.945 (0.006)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XNgxP3B3s-Px"},"source":["# Set the parameters by cross-validation\n","def cross_validate():\n","  tuned_parameters = [{'loss': ['hinge', 'squared_hinge'],\n","                      'C': [1, 10, 20, 100, 1000],\n","                      'dual': [True,False],\n","                      'tol': [0.001, 0.0001, 0.00001, 0.000001],\n","                      'max_iter': [10, 50, 100, 1000, 10000]\n","                      }]\n","\n","  clf = GridSearchCV(\n","    LinearSVC(), tuned_parameters, scoring='accuracy', cv=skf, verbose=2\n","  )\n","  clf.fit(training_features, df_train['target'])\n","\n","  print(\"Best parameters set found on development set:\")\n","  print()\n","  print(clf.best_params_)\n","  print()\n","  print(\"Grid scores on development set:\")\n","  print()\n","  means = clf.cv_results_['mean_test_score']\n","  stds = clf.cv_results_['std_test_score']\n","  for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n","    print(\"%0.3f (+/-%0.03f) for %r\"\n","          % (mean, std * 2, params))\n","  print()\n","\n","  print(\"Detailed classification report:\")\n","  print()\n","  print(\"The model is trained on the full development set.\")\n","  print(\"The scores are computed on the full evaluation set.\")\n","  print()\n","  y_true, y_pred = df_test['target'], clf.predict(testing_features)\n","  print(classification_report(y_true, y_pred))\n","  print()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZHLaNBdFdG4v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611606475283,"user_tz":-60,"elapsed":351,"user":{"displayName":"Gabriele Calarota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLOMt6Niz_X342oKHc1h7NaiYbCz3ctbWwY1-Bf_A=s64","userId":"12128926370618990508"}},"outputId":"4aea251d-64e9-475b-9700-ac303325ba49"},"source":["!pip install -q python-telegram-bot\n","from telegram import Bot\n","bot = Bot(token=\"627493222:AAE8dqAHnrx9JJ3AGxDwb-x2eiJqoXVBM8o\")\n","bot.send_message(text=\"Training finito di SVM {} on {} authors with {} ndocs and {} threshold\".format(PROJECT_NAME, NUM_AUTHORS, N_DOCS, N_THRESHOLD), chat_id=\"141928344\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 430kB 5.0MB/s \n","\u001b[K     |████████████████████████████████| 61kB 5.8MB/s \n","\u001b[K     |████████████████████████████████| 2.6MB 9.4MB/s \n","\u001b[?25h"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<telegram.message.Message at 0x7ffad0ae65f8>"]},"metadata":{"tags":[]},"execution_count":0}]}]}